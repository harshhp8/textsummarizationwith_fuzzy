{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc60900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H:/College WOrk/papers/stop-words.txt',encoding='utf-16') as fp:\n",
    "    stop_words = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1612c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_document = ctrl.Antecedent(np.arange(0, 1.25, .1), 'similarity')\n",
    "senlength = ctrl.Antecedent(np.arange(0, 1.25, .1), 'senlength')\n",
    "senpos=ctrl.Antecedent(np.arange(0, 2, .1), 'senpos')\n",
    "nounplace=ctrl.Antecedent(np.arange(0, 1.2, .1), 'nounplace')\n",
    "pronounplace=ctrl.Antecedent(np.arange(0, 1.2, .1), 'pronounplace')\n",
    "tfidfscore = ctrl.Antecedent(np.arange(0, 1.25, .1), 'tfidfscore')\n",
    "\n",
    "\n",
    "#consequent\n",
    "rank = ctrl.Consequent(np.arange(0, 24, 1), 'rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a696b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_document.automf(3)\n",
    "senlength.automf(3)\n",
    "senpos.automf(3)\n",
    "nounplace.automf(3)\n",
    "pronounplace.automf(3)\n",
    "tfidfscore.automf(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86942d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#consequent\n",
    "rank['low']=fuzz.trimf(rank.universe,[0,0,10])\n",
    "rank['medium']=fuzz.trimf(rank.universe,[0,10,24])\n",
    "rank['high']=fuzz.trimf(rank.universe,[10,24,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e455270",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule1 = ctrl.Rule(similarity_document['poor'] & senlength['poor'] & senpos['poor'] & nounplace['poor'] & pronounplace['poor'] & tfidfscore['poor']  , rank['low'])\n",
    "rule2 = ctrl.Rule(similarity_document['average'] & senlength['average'] & senpos['average'] & nounplace['average'] & pronounplace['average'] & tfidfscore['average'], rank['medium'])\n",
    "rule3 = ctrl.Rule(similarity_document['good'] & senlength['good'] & senpos['good']& nounplace['good'] & pronounplace['good'] & tfidfscore['good'] , rank['high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5132d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankFIS = ctrl.ControlSystem([rule1,rule2,rule3])\n",
    "rankFIS = ctrl.ControlSystemSimulation(rankFIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def tokenizefortfidf(originalsen):\n",
    "    tokenized_sents = [word_tokenize(i) for i in originalsen]\n",
    "    sen_list1=[]\n",
    "    for i in tokenized_sents:\n",
    "        sen_list1=sen_list1+i\n",
    "    return sen_list1\n",
    "#print(sen_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return dict(list(zip(wordlist,wordfreq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortFreqDict(freqdict):\n",
    "    aux = [(freqdict[key], key) for key in freqdict]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb306f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    for word, val in docList.items():\n",
    "        idfDict[word] = math.log10(N / float(val) + 1)\n",
    "        \n",
    "    return(idfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64210d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string   \n",
    "    return (str1.join(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096f77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "def loopsecond(word1len,word1,idf,j):\n",
    "    for k in range(word1len): \n",
    "            if idf.index[j] == word1[k]:\n",
    "                lst.append(idf['ratio'][j])\n",
    "                break\n",
    "\n",
    "def loopfirst(leng,word1,idf):\n",
    "    word1len = len(word1)\n",
    "    for j in range(leng):\n",
    "        #print(j)\n",
    "        loopsecond(word1len,word1,idf,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('indian')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e2bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import tnt \n",
    "from nltk.corpus import indian \n",
    "\n",
    "#train_data = indian.tagged_sents('gu.pos')\n",
    "train_data = indian.tagged_sents('C:/Users/HM-AM/Documents/Jupyter Notebooks/gu.pos')\n",
    "tnt_pos_tagger = tnt.TnT() \n",
    "tnt_pos_tagger.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af959e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(train_data) * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sents = train_data\n",
    "train_sents = train_data[:size]\n",
    "test_sents = train_data[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_tagger(train_sentences, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sentences, backoff=backoff)\n",
    "        \n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_tagger = nltk.DefaultTagger('NN')\n",
    "Combine_tagger = backoff_tagger(train_sents,\n",
    "[nltk.UnigramTagger, nltk.BigramTagger, nltk.TrigramTagger], backoff = back_tagger)\n",
    "\n",
    "Combine_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(org_list, seperator=' '):\n",
    "    \"\"\" Convert list to string, by joining all item in list with given separator.\n",
    "        Returns the concatenated string \"\"\"\n",
    "    return seperator.join(org_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def Average(lst):\n",
    "    return mean(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge.rouge import rouge_n_sentence_level\n",
    "from rouge import rouge_n_summary_level\n",
    "from rouge import rouge_l_summary_level\n",
    "from rouge import rouge_w_sentence_level\n",
    "from rouge import rouge_w_summary_level\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "def comparerouge1(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a1_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "    _, _, rouge_1 = rouge_n_summary_level(tokenize_words, tokenize_words1, 1)\n",
    "    #rouge1.append(rouge_1)\n",
    "    #print('ROUGE-1: %f' % rouge_1)\n",
    "    _, _, rouge_2 = rouge_n_summary_level(tokenize_words, tokenize_words1, 2)\n",
    "    #print('ROUGE-2: %f' % rouge_2)\n",
    "    #rouge2.append(rouge_2)\n",
    "    _, _, rouge_l = rouge_l_summary_level(tokenize_words, tokenize_words1)\n",
    "    #print('ROUGE-L: %f' % rouge_l)\n",
    "    #rougel.append(rouge_l)\n",
    "    _, _, rouge_w = rouge_w_summary_level(tokenize_words, tokenize_words1)\n",
    "    #avg=(rouge_1+rouge_2+rouge_l+rouge_w)/4\n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparerouge2(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a1_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "   \n",
    "    _, _, rouge_2 = rouge_n_summary_level(tokenize_words, tokenize_words1, 2)\n",
    "   \n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f23a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparerougel(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a1_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "    \n",
    "    _, _, rouge_l = rouge_l_summary_level(tokenize_words, tokenize_words1)\n",
    "    \n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_l    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fecf9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparerougew(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a1_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "    _, _, rouge_w = rouge_w_summary_level(tokenize_words, tokenize_words1)\n",
    "    \n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H:/College WOrk/papers/stop-words.txt',encoding='utf-16') as fp:\n",
    "    v = fp.read()\n",
    "\n",
    "def removestopwords(line):\n",
    "    #print(\"original line\",line)\n",
    "    querywords = line.split()\n",
    "\n",
    "    resultwords  = [word for word in querywords if word.lower() not in v]\n",
    "    result = ' '.join(resultwords)\n",
    "    #print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd119fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from indicnlp.tokenize import sentence_tokenize\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "import math\n",
    "import io\n",
    "\n",
    "nounlist=['N-NNP','N_NN','N_NNV','N_NST','NN']\n",
    "pronounlst=['PR_PRC','PR_PRF','PR_PRI','PR_PRP','PR_PRQ']\n",
    "fc=[]\n",
    "li=[]\n",
    "maintext=[]\n",
    "rouge1=[]\n",
    "rouge2=[]\n",
    "rougel=[]\n",
    "rougew=[]\n",
    "class ShortText:\n",
    "    def __init__(self, my_id, human_label, summary, short_text):\n",
    "        self.id = my_id         \n",
    "        self.human_label = human_label    \n",
    "        self.summary = summary \n",
    "        self.short_text = short_text\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        For printing purposes.\n",
    "        '''\n",
    "        return '%d\\t%d\\t%s\\t%s' % (self.id, self.human_label, self.summary, self.short_text)\n",
    "\n",
    "def load_file8(filename,createfile):\n",
    "    tokenize_words=[]\n",
    "    titlesentences=[]\n",
    "    file1 = open(createfile, \"w\", encoding=\"utf-8\")\n",
    "    #retrieve the original text \n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    soup = BeautifulSoup(data)\n",
    "    docno=soup.find_all('docno')\n",
    "    text=soup.find_all('text')\n",
    "    titleno = soup.find_all('title')\n",
    "    instances = {}\n",
    "    my_id = 0\n",
    "    for n,tit,maintitle in zip(docno,text,titleno):\n",
    "        print(\"This is doc file\",n)\n",
    "        count=0\n",
    "        tit=tit.get_text()\n",
    "        tit=str(tit)\n",
    "        #print(\"this is text\",tit)\n",
    "        maintitle=maintitle.get_text()\n",
    "        maintitle=str(maintitle)\n",
    "        maintitlelist=[]\n",
    "        for tokenize in indic_tokenize.trivial_tokenize(maintitle):\n",
    "            titlesentences.append(tokenize)\n",
    "        list8 = [''.join(c for c in s if c not in string.punctuation) for s in titlesentences]\n",
    "        list8 = [s for s in list8 if s]\n",
    "        originalsen=[]\n",
    "        sentences=sentence_tokenize.sentence_split(tit, lang='gu')\n",
    "        totalsen=len(sentences)\n",
    "        #print(sentences)\n",
    "        list3 = [''.join(c for c in s if c not in string.punctuation) for s in sentences]\n",
    "        list3 = [s for s in list3 if s]\n",
    "        #print(\"this is list3\",list3)\n",
    "        dictionary = wordListToFreqDict(list3)\n",
    "        sorteddict = sortFreqDict(dictionary)\n",
    "        context=listToString(list3)\n",
    "        wordDictA = dict.fromkeys(list3, 0)\n",
    "        for word in list3:\n",
    "            wordDictA[word]+=1\n",
    "        df2=pd.DataFrame([wordDictA])\n",
    "        tfFirst = computeTF(wordDictA, context)\n",
    "        tf = pd.DataFrame([tfFirst])\n",
    "        idfs = computeIDF(wordDictA)\n",
    "        idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "        #idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "        #putting it in a dataframe\n",
    "        idf= pd.DataFrame([idfFirst])\n",
    "        idf = idf.transpose()\n",
    "        idf.columns=['ratio']\n",
    "        final_df = pd.DataFrame(columns=['sentence', 'score'])\n",
    "        for i in range(len(list3)):\n",
    "            list3[i]=str(list3[i])[1:-1]\n",
    "            word = listToString(list3[i])\n",
    "   \n",
    "            word1 = word.split()\n",
    "    \n",
    "            lenofdf=len(idf)\n",
    "            loopfirst(lenofdf,word1,idf)\n",
    "            total = sum(lst)+1\n",
    "            final_df = final_df.append({'sentence': list3[i], 'score': total},ignore_index='True')\n",
    "    \n",
    "            lst.clear()\n",
    "        #print(\"this is final_df\",final_df)\n",
    "        lengthoflongestsen=len(max(sentences, key=len))\n",
    "        #print(lengthoflongestsen)\n",
    "        #list3 = [s for s in sentences if s]\n",
    "        df_fuzzy = pd.DataFrame(columns=['sentence', 'score'])\n",
    "        for index, t in enumerate(list3):\n",
    "            nounplace=0\n",
    "            pronounplace=0\n",
    "            #print(\"this is docno\",n)\n",
    "            sen=removestopwords(t)\n",
    "            res_list = [SequenceMatcher(None, sen, list3[i]).ratio() for i in range(len(list3)) if i!=index]\n",
    "            average = Average(res_list)\n",
    "            rankFIS.input['similarity'] = average            \n",
    "            senlength=len(t)/lengthoflongestsen\n",
    "            rankFIS.input['senlength']=senlength\n",
    "            \n",
    "            if index==0:\n",
    "                senpos=0+0.1\n",
    "            elif index==len(sentences)-1:\n",
    "                senpos=1\n",
    "            else:\n",
    "                senpos=index/totalsen\n",
    "            #print(\"this is senpos\",senpos)\n",
    "            rankFIS.input['senpos']=senpos\n",
    "            \n",
    "            tagged_words = (Combine_tagger.tag(nltk.word_tokenize(t)))\n",
    "            lst1=[]\n",
    "            for w in tagged_words:\n",
    "                lst1.append(w[1])\n",
    "            for w in lst1:\n",
    "                if w in nounlist:\n",
    "                    nounplace=1\n",
    "            #print(\"this is nounplace\",nounplace)\n",
    "            rankFIS.input['nounplace']=nounplace\n",
    "            \n",
    "            for w1 in lst1:\n",
    "                if w1 in pronounlst:\n",
    "                    pronounplace=1          \n",
    "            #print(\"this is pronounplace\",pronounplace)\n",
    "            rankFIS.input['pronounplace']=pronounplace+0.1\n",
    "            #print(final_df.loc[index]['score'])\n",
    "            rankFIS.input['tfidfscore']=final_df.loc[index]['score']\n",
    "            rankFIS.compute()\n",
    "#             print (\"the answer is\")\n",
    "#             print (rankFIS.output['rank']) \n",
    "            df_fuzzy = df_fuzzy.append({'sentence': t, 'score': rankFIS.output['rank']},ignore_index='True')\n",
    "        df_fuzzy.sort_values(by=['score'], inplace=True, ascending=False)\n",
    "        df_fuzzy2=df_fuzzy['sentence'].head(round(20*len(df_fuzzy.index)/100))\n",
    "        listtoretrieve=list(df_fuzzy2.index.values) \n",
    "        listtoretrieve.sort()\n",
    "        df2 = pd.DataFrame(list3,columns=['sentences'])\n",
    "        new_list = df2.values.tolist()\n",
    "        res1=[]\n",
    "\n",
    "        for x in new_list:\n",
    "            res1=res1+x\n",
    "        finaldraft=\"\"\n",
    "        for i in listtoretrieve:\n",
    "            finaldraft+=res1[i]\n",
    "        finaldraft=\" \".join(finaldraft.split())\n",
    "        rouge_1=comparerouge1(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/sports/annotator1/\",finaldraft,filename)\n",
    "        rouge_2=comparerouge2(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/sports/annotator1/\",finaldraft,filename)\n",
    "        rouge_l=comparerougel(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/sports/annotator1/\",finaldraft,filename)\n",
    "        rouge_w=comparerougew(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/sports/annotator1/\",finaldraft,filename)\n",
    "        \n",
    "        rouge1.append(rouge_1)\n",
    "        rouge2.append(rouge_2)\n",
    "        rougel.append(rouge_l)\n",
    "        rougew.append(rouge_w)\n",
    "        \n",
    "        rouge1average=Average(rouge1)\n",
    "        rouge2average=Average(rouge2)\n",
    "        rougelaverage=Average(rougel)\n",
    "        rougewaverage=Average(rougew)\n",
    "        print(\"rouge1 average\",rouge1average)\n",
    "        print(\"rouge2 average\",rouge2average)\n",
    "        print(\"rougel average\",rougelaverage)\n",
    "        print(\"rougew average\",rougewaverage)\n",
    "        #print(finaldraft)\n",
    "        with io.open(createfile, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(finaldraft)\n",
    "        instances = {}\n",
    "    \n",
    "        \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d10cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#for root, dirs, files in os.walk(\"/home/harsh/Documents/gujdocs/gsf2003/\"):\n",
    "for root, dirs, files in os.walk(\"G:/Movies/gujarati Text summarization dataset/sports/\"):    \n",
    "#for root, dirs, files in os.walk(\"G:/Movies/post_1/\"):    \n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(root, file), 'r') as f:\n",
    "                #print(os.path.join(root, file))\n",
    "                load_file8(os.path.join(root, file),\"G:/Movies/gujarati Text summarization datasetfuzzy/sports/\"+file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
